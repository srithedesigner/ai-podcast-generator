import os
from openai import OpenAI
from dotenv import load_dotenv
import requests
import json
import fal_client

load_dotenv()


def llm_service(prompt: str) -> str:
    """
    Processes a given prompt using the OpenAI API
    and returns the model's output.

    Args:
        prompt: The input string to be sent to the LLM.

    Returns:
        The output string generated by the LLM.
    """
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            return "Error: OPENAI_API_KEY environment variable not set."

        client = OpenAI(api_key=api_key)

        response = client.chat.completions.create(
            model="o4-mini-2025-04-16",  # Or any other model you prefer
            messages=[
            {"role": "user", "content": prompt}
            ]
        )
        
        if response.choices:
            output = response.choices[0].message.content
            return output.strip() if output else "Error: Empty response from LLM."
        else:
            return "Error: No response choices from LLM."

    except Exception as e:
        return f"Error interacting with OpenAI API: {e}"



def on_queue_update(update):
    if isinstance(update, fal_client.InProgress):
        for log in update.logs:
            print(log["message"])

def generate_image_from_prompt(prompt: str):
    """
    Generates an image based on the input prompt using fal-ai/flux-pro/v1.1.

    Args:
        prompt: The text prompt to generate the image from.

    Returns:
        The result from the fal_client.subscribe call, which typically
        contains information about the generated image.
    """
    podcast_prompt = "hyper-realistic image of the the subject sitting at a podcast studio table. The character is seated upright in a red chair, facing the camera directly, with hands folded neatly on a light wooden podcast desk. They are wearing large, black over-ear studio headphones. A professional condenser microphone on a boom arm is visible on the left side"

    prompt = f"{podcast_prompt}\n\n character description d{prompt}"
    try:
        result = fal_client.subscribe(
            "fal-ai/flux-pro/v1.1",  # Using a specific model version
            arguments={
                "prompt": prompt
            },
            with_logs=True,
            on_queue_update=on_queue_update,
        )
        return result
    except Exception as e:
        print(f"An error occurred: {e}")
        return None

